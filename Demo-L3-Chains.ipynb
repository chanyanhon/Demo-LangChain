{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b34cede2-3954-497f-88d8-047f348464f3",
   "metadata": {},
   "source": [
    "# Chains in LangChain\n",
    "\n",
    "## Objective\n",
    "* You can use multiple prompts to ingest the input variale step by step to perform different actions. With only one prompt focus on one action at one time, target result will be generated more accurately and with higher consistency, compared to one prompt generating information in multiple aspects, due to NLP attention mechanism.\n",
    "* You also can choose different prompts for next step based on the analyzed resuts of previous prompts. Say analyzing Black Hole by a physics-related prompt and analyzing history by a history-related prompt, with a classification prompt to classify the subject of the input vairables\n",
    "\n",
    "## Outline\n",
    "\n",
    "* LLMChain\n",
    "* Sequential Chains\n",
    "* Router Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61c2f31a-6202-430f-a025-cbaa8605f915",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q python-dotenv\n",
    "!pip install -q openai==0.28.1\n",
    "!pip install -q --upgrade langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b4ba5b-8f2f-43a1-96cf-1ae638dafdc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Get your [OpenAI API Key](https://platform.openai.com/account/api-keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d4b1ba9-b954-4867-ac60-a4bb0c3e0099",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Get OpenAI API key\n",
    "try:\n",
    "    _ = load_dotenv(find_dotenv()) # read local .env file\n",
    "    openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "except:\n",
    "    openai.api_key = 'your openai key'\n",
    "\n",
    "# Get OpenAI Model type\n",
    "llm_model = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3775ff-5028-4dc0-bf0d-c10e144f58de",
   "metadata": {},
   "source": [
    "Note: LLM's do not always produce the same results. When executing the code in your notebook, you may get slightly different answers that those in the video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb15b1d-4806-4bbe-a8c1-9f8ebad5d49e",
   "metadata": {},
   "source": [
    "## LLMChain\n",
    "\n",
    "Recall for using input variables instead of text concatentation as input prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a7a02c0-b5e1-4c44-a450-c9ac7d6dba6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf89ae20-15b8-42cc-9099-747586f14a68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Royal Comfort Bedding'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature=0.9, model=llm_model)\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}?\"\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "product = \"Queen Size Sheet Set\"\n",
    "chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e30606-551d-49d3-8c64-2b294c5f5236",
   "metadata": {},
   "source": [
    "## SequentialChain\n",
    "\n",
    "Provided with input for the first chain, the first chain will output vairables and we can define the name of that.\n",
    "\n",
    "For a chain with multiple sub-chains, the variables output will be stored which can be used in later chain, e.g.\n",
    "- chain_two output \"summary\" variable, which is used in chain_four\n",
    "\n",
    "Finally, the SequentialChain can output with JSON format for all input & output variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "515e0821-d850-4416-8bc6-4a30fd5820dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92ec544f-0a31-4474-87a9-842fac2d17c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9, model=llm_model)\n",
    "\n",
    "# prompt template 1: translate to english\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate the following review to english:\"\n",
    "    \"\\n\\n{Review}\"\n",
    ")\n",
    "# chain 1: input= Review and output= English_Review\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt, \n",
    "                     output_key=\"English_Review\"\n",
    "                    )\n",
    "\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Can you summarize the following review in 1 sentence:\"\n",
    "    \"\\n\\n{English_Review}\"\n",
    ")\n",
    "# chain 2: input= English_Review and output= summary\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt, \n",
    "                     output_key=\"summary\"\n",
    "                    )\n",
    "\n",
    "# prompt template 3: translate to english\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What language is the following review:\\n\\n{Review}\"\n",
    ")\n",
    "# chain 3: input= Review and output= language\n",
    "chain_three = LLMChain(llm=llm, prompt=third_prompt,\n",
    "                       output_key=\"language\"\n",
    "                      )\n",
    "\n",
    "# prompt template 4: follow up message\n",
    "fourth_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a follow up response to the following \"\n",
    "    \"summary in the specified language:\"\n",
    "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
    ")\n",
    "# chain 4: input= summary, language and output= followup_message\n",
    "chain_four = LLMChain(llm=llm, prompt=fourth_prompt,\n",
    "                      output_key=\"followup_message\"\n",
    "                     )\n",
    "\n",
    "# overall_chain: input= Review \n",
    "# and output= English_Review,summary, followup_message\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
    "    input_variables=[\"Review\"],\n",
    "    output_variables=[\"English_Review\", \"summary\",\"followup_message\"],\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d890f2c-3635-4008-acd5-87bb2212bdfb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Review': '\\n\"Je trouve le goût médiocre. La mousse ne tient pas, c\\'est bizarre. J\\'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\"\\n',\n",
       " 'English_Review': '\"I find the taste mediocre. The foam doesn\\'t last, it\\'s strange. I buy the same ones in stores and the taste is much better... Old batch or counterfeit!?\"',\n",
       " 'summary': \"The reviewer is disappointed in the taste of the product, noting that the foam doesn't last and suspects that it may be an old batch or counterfeit.\",\n",
       " 'followup_message': \"Merci de nous avoir fait part de votre déception concernant le goût du produit. Votre avis est très important pour nous. Nous vous assurons que nous prendrons des mesures pour enquêter sur la qualité du lot en question et nous assurer que nos produits sont authentiques. Votre satisfaction est notre priorité et nous espérons pouvoir regagner votre confiance à l'avenir. Merci de votre fidélité.\"}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = '''\n",
    "\"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\"\n",
    "'''\n",
    "output = overall_chain(review)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e369c29-0491-423a-8702-2b624923ea90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9003e839-b261-4855-844c-66a4f67b20a8",
   "metadata": {},
   "source": [
    "## Router Chain\n",
    "\n",
    "Provided with \n",
    "- classification prompt to classify the subject of the input variable in step 1\n",
    "- multiple prompts to be chosen in step 2 based on result of step 1\n",
    "\n",
    "input for the first chain, the first chain will output vairables and we can define the name of that.\n",
    "\n",
    "For a chain with multiple sub-chains, the variables output will be stored which can be used in later chain, e.g.\n",
    "- chain_two output \"summary\" variable, which is used in chain_four\n",
    "\n",
    "Finally, the SequentialChain can output with JSON format for all input & output variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cfb7afc9-b798-4307-aa45-e5fb44551f93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# multiple prompts to be chosen\n",
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise\\\n",
    "and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit\\\n",
    "that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. \\\n",
    "You are great at answering math questions. \\\n",
    "You are so good because you are able to break down \\\n",
    "hard problems into their component parts, \n",
    "answer the component parts, and then put them together\\\n",
    "to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "history_template = \"\"\"You are a very good historian. \\\n",
    "You have an excellent knowledge of and understanding of people,\\\n",
    "events and contexts from a range of historical periods. \\\n",
    "You have the ability to think, reflect, debate, discuss and \\\n",
    "evaluate the past. You have a respect for historical evidence\\\n",
    "and the ability to make use of it to support your explanations \\\n",
    "and judgements.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
    "You have a passion for creativity, collaboration,\\\n",
    "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
    "understanding of theories and algorithms, and excellent communication \\\n",
    "skills. You are great at answering coding questions. \\\n",
    "You are so good because you know how to solve a problem by \\\n",
    "describing the solution in imperative steps \\\n",
    "that a machine can easily interpret and you know how to \\\n",
    "choose a solution that has a good balance between \\\n",
    "time complexity and space complexity. \n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b619ecfe-2a02-4a12-84e9-9986b4f568ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"physics\", \n",
    "        \"description\": \"Good for answering questions about physics\", \n",
    "        \"prompt_template\": physics_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"math\", \n",
    "        \"description\": \"Good for answering math questions\", \n",
    "        \"prompt_template\": math_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"History\", \n",
    "        \"description\": \"Good for answering history questions\", \n",
    "        \"prompt_template\": history_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"computer science\", \n",
    "        \"description\": \"Good for answering computer science questions\", \n",
    "        \"prompt_template\": computerscience_template\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af61d40f-031a-4d84-bbaa-84703c560f4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c584cd8d-8e58-4cc6-a6fe-beab89ed7584",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate destination_chains and destinations_str\n",
    "llm = ChatOpenAI(temperature=0, model=llm_model)\n",
    "\n",
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain  \n",
    "    \n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3c71b59-9d4e-4adc-9656-72c7b92a379e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate a default chain if provided prompts in step 2 are not appropriate to be used\n",
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25b9d0f6-a6e3-478e-b557-a8ac1713dbce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# classifcation prompt\n",
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
    "language model select the model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a \\\n",
    "description of what the prompt is best suited for. \\\n",
    "You may also revise the original input if you think that revising\\\n",
    "it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
    "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
    "well suited for any of the candidate prompts.\n",
    "REMEMBER: \"next_inputs\" can just be the original input \\\n",
    "if you don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d9f01b79-5036-493b-adae-064c7b0b657d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# input \"destinations_str\" as \"destinations\" under << CANDIDATE PROMPTS >>\n",
    "# to let LLM know the description of LLM chain capability and \n",
    "# output the name of the prompt\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinations_str\n",
    ")\n",
    "\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d6cbda0-45eb-4e93-b5d0-7d2d419abca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Router_chain will output \n",
    "#   1. name of the prompt, and\n",
    "#   2. modified version of the original input\n",
    "# MultiPromptChain based on the Router_chain output (classification prompt), choose the optimal key-value pair \n",
    "# among destination_chains choies (i.e. physics, math, history, computer science)\n",
    "# if not belongs to provided choices, it output \"DEFAULT\" and use default_chain\n",
    "chain = MultiPromptChain(router_chain=router_chain, \n",
    "                         destination_chains=destination_chains, \n",
    "                         default_chain=default_chain, verbose=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf125ed2-5e0b-4a5b-95a9-cee1f90933d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "physics: {'input': 'What is black body radiation?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Black body radiation refers to the electromagnetic radiation emitted by a perfect black body, which is an object that absorbs all radiation that falls on it and emits radiation at all wavelengths. The radiation emitted by a black body depends only on its temperature and follows a specific distribution known as Planck's law. This type of radiation is important in understanding the behavior of stars, as well as in the development of technologies such as incandescent light bulbs and infrared cameras.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"What is black body radiation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5255cfed-95b8-4468-b0d0-3a7658d0054b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n    \"destination\": \"physics\",\\n    \"next_inputs\": \"What is black body radiation?\"\\n}\\n```'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## to undermore more what the classifiation chain return\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.9, model=llm_model)\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=router_prompt)\n",
    "product = \"What is black body radiation?\"\n",
    "chain.run(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5097c77a-a691-4a73-8e0b-84c400c94616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "math: {'input': 'what is 2 + 2'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'As an AI language model, I can answer this question easily. The answer to 2 + 2 is 4.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"what is 2 + 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "266cc0a5-4a8b-4c2f-a9ec-e3cf386b6b5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n    \"destination\": \"biology\",\\n    \"next_inputs\": \"Why does every cell in our body contain DNA?\"\\n}\\n```'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"Why does every cell in our body contain DNA?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa97420-e89c-431d-8e62-2d29bcd9e048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a345144-102d-4c4f-9939-a2b16fe01e7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7286978-e4fe-4d2a-b2ba-e93a79f94e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3272ef1a-8e26-415c-9cc3-dfb2bfd05b9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Summary\n",
    "\n",
    "## 1. LLMChain\n",
    "\n",
    "In the context of text manipulation and integration with libraries, LangChain adopt a different approach which are better in terms of scalability.\n",
    "\n",
    "### Prompt praser Approach\n",
    "ChatGPT primarily relies on f-strings for text manipulation, which can become cumbersome, especially in complex scenarios involving multiple variables. For example:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "\n",
    "```python\n",
    "service_messages = prompt_template.format_messages(\n",
    "    style=service_style_pirate,\n",
    "    text=service_reply)\n",
    "```\n",
    "\n",
    "### LLMChain Approach\n",
    "LangChain simplifies scalability by directly interpreting variables stored within its library, eliminating the need for f-strings. This approach enhances code readability and maintainability, particularly in large-scale projects.xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "\n",
    "Similar to prompt praser in L1, LLMChain can directly query the variable\n",
    "\n",
    "```python\n",
    "LLMChain:\n",
    "product = \"Queen Size Sheet Set\"\n",
    "LLMChain(llm=llm, prompt=prompt).run(product)\n",
    "```\n",
    "\n",
    "\n",
    "## 2. SequentialChain\n",
    "\n",
    "We can chain the LLMChain step by step. We can define the output vairable and stored for future chains. Finally output variables in JSON format.\n",
    "\n",
    "Avoiding storing each output from ChatGPT, SequentialChain provide a more lean solution which you can manipulate the input and output from all processing depends on others.\n",
    "(Airflow of chatgpt pipeline)\n",
    "\n",
    "With single chain performing single action at a time, due to Attention mechanism for NLP Attention model (e.g.LLM), the accuracy and consistency of LLM will be higher.\n",
    "\n",
    "\n",
    "```python\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
    "    input_variables=[\"Review\"],\n",
    "    output_variables=[\"English_Review\", \"summary\",\"followup_message\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "overall_chain(review)\n",
    "```\n",
    "\n",
    "## Conclusion\n",
    "In conclusion, while both ChatGPT and LangChain serve text manipulation purposes, LangChain's innovative approach to scalability and output consistency, particularly its direct interpretation of variables and structured output handling, positions it as a preferred choice for large-scale projects and maintaining pipeline integrity. By addressing challenges such as inconsistent JSON formatting and key misalignment, LangChain offers a robust solution for seamless integration within text processing pipelines.xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6a0adc-2754-4586-b9ca-d23e3a9daf09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec65d5c-0e55-4837-beb7-8c0923b865e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
